# üìù Preprints

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/publication/ck_arena.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs**

Shuhang Xu, Weijing Deng, Yixuan Zhou, **Fangwei Zhong&#x2709;**

***ArXiv preprint, 2025***

[Project](https://ck-arena.site/), [Paper](https://arxiv.org/pdf/2505.17512)
- A scalable and game-based benchmark for assessing the built-in conceptual knowledge of LLMs.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/publication/hievt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Hierarchical Instruction-aware Embodied Visual Tracking**

Kui Wu, Hao Chen, Churan Wang, Fakhri Karray, Zhoujun Li, Yizhou Wang, **Fangwei Zhong&#x2709;**

***ArXiv preprint, 2025***

[Project](https://sites.google.com/view/hievt), [Paper](https://arxiv.org/abs/2505.20710)
- A hierarchical framework that combines LLM-based semantic-spatial goal aligner and RL-based adaptive goal-aligned policy for user-centric embodied visual tracking.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/publication/adv_grasp.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Grasparl: Dynamic Grasping via Adversarial Reinforcement Learning**

Tianhao Wu\*, **Fangwei Zhong**\*, Yiran Geng, Hongchen Wang, Yongjian Zhu, Yizhou Wang, Hao Dong

***ArXiv preprint, 2022***

[Paper](https://arxiv.org/pdf/2203.02119)
- Learning to grasp moving objects with vision observation via adversarial RL.
</div>
</div>

# üìù Publications 

\* : co-first author, &#x2709; : corresponding author

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/publication/unrealzoo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI**

**Fangwei Zhong**\* &#x2709;, Kui Wu\*, Churan Wang, Hao Chen, Hai Ci, Zhoujun Li, Yizhou Wang

***International Conference on Computer Vision (ICCV), 2025 <span style="color:red">(Highlight)</span>***

[Project](http://unrealzoo.site/), [Paper](https://arxiv.org/abs/2412.20977), [Code ![code](https://img.shields.io/github/stars/UnrealZoo/unrealzoo-gym?style=social&label=Code+Stars)](https://github.com/UnrealZoo/unrealzoo-gym)
- A rich collection of photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of the open worlds.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IROS 2025</div><img src='images/publication/vlm_track.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Vision-Language Models**

Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang, **Fangwei Zhong&#x2709;**

***The 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025***

[Project](https://sites.google.com/view/evt-recovery-assistant), [Paper](https://arxiv.org/abs/2505.20718)
- A self-improving framework that enhances Embodied Visual Tracking (EVT) with Vision-Language Models (VLMs) to recover tracking from failure.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/publication/ACL25_main.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
**CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games**

Shuhang Xu, **Fangwei Zhong&#x2709;**

***The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025 (<span style="color:red">Oral, SAC Highlight Award</span>)***

[Paper](https://www.arxiv.org/abs/2505.18218), [Code](https://github.com/Yeswolo/CoMet)
- A self-improving reasoning framework that enables LLM-based agents to engage in metaphor processing on Undercover and Adversarial Taboo.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/publication/ACL25_findings.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
**Are the Values of LLMs Structurally Aligned with Humans? A Causal Perspective**

Yipeng Kang, Junqi Wang, Yexin Li, Mengmeng Wang, Wenming Tu, Quansen Wang, Hengli Li, Tingjun Wu, Xue Feng, **Fangwei Zhong**, Zilong Zheng&#x2709;

***The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025 (Findings)***

[Paper](https://arxiv.org/abs/2501.00581)
- Discovering the latent causal structure of the values in LLMs.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2025</div><img src='images/publication/icml25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
**Behavior-agnostic Task Inference for Robust Offline In-context Reinforcement Learning**

Long Ma, **Fangwei Zhong&#x2709;**, Yizhou Wang

***International Conference on Machine Learning (ICML), 2025***

[Project](https://sites.google.com/view/bati-icrl), [Paper](https://openreview.net/pdf?id=jMKaATBEKb)
- A model-based task inference method that is robust to the changes of in-context behavior for offline RL.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/publication/d2a.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
**Simulating Human-like Daily Activities with Desire-driven Autonomy**

Yiding Wang\*, Yuxuan Chen\*, **Fangwei Zhong**&#x2709;, Long Ma, Yizhou Wang

***International Conference on Learning Representations (ICLR), 2025***

[Project](https://sites.google.com/view/desire-driven-autonomy), [Paper](https://arxiv.org/abs/2412.06435), [Code](https://github.com/zfw1226/D2A)
- A desire-driven autonomy framework to guide LLM-based agents to simulate human-like daily activities in text-based environments.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/publication/Richelieu.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy**

Zhenyu Guan, Xiangyu Kong&#x2709;, **Fangwei Zhong&#x2709;**, Yizhou Wang

***Advances in Neural Information Processing Systems (NeurIPS), 2024***

[Project](https://sites.google.com/view/richelieu-diplomacy), 
[Paper](https://openreview.net/pdf?id=7Jb4NJS8Yk)
-  A LLM-based social agent that can solve the game of AI Diplomacy only by self-play, without fine-tuning or regularizing on human data.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/publication/usim.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Fine-Tuning Out-of-Vocabulary Item Recommendation with User Sequence Imagination**

Ruochen Liu, Hao Chen, Yuanchen Bei, Qijie Shen, **Fangwei Zhong**, Senzhang Wang, Jianxin Wang

***Advances in Neural Information Processing Systems (NeurIPS), 2024 <span style="color:red">(Spotlight)</span>***

[Paper](https://openreview.net/pdf?id=JyWAFGCJPl)
- A novel User Sequence IMagination (USIM) fine-tuning framework that first
imagines the user sequences and then refines the generated OOV embeddings
with the user behavioral embeddings.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/publication/eccv2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL**

**Fangwei Zhong**\*, Kui Wu\*, Hai Ci, Churan Wang, Hao Chen

***The 18th European Conference on Computer Vision (ECCV), 2024***

[Project](https://sites.google.com/view/offline-evt), 
[Paper](https://arxiv.org/pdf/2404.09857)
- Significantly improved the training efficiency and generalization of embodied visual tracking with visual foundation models and offline RL.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/publication/icml24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Fast Peer Adaptation with Context-aware Exploration**

Long Ma\*, Yuanfei Wang\*, **Fangwei Zhong&#x2709;**, Song-Chun Zhu, Yizhou Wang

***International Conference on Machine Learning (ICML), 2024***

[Project](https://sites.google.com/view/peer-adaptation), 
[Paper](https://arxiv.org/pdf/2402.02468)
- Learn a context-aware policy with a peer identification reward to effectively explore and quickly adapt to unknown peers. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE T-PAMI</div><img src='images/publication/bi-dexhands.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Bi-dexhands: Towards Human-level Bimanual Dexterous Manipulation**

Yuanpei Chen, Yiran Geng, **Fangwei Zhong**, Jiaming Ji, Jiechuang Jiang, Zongqing Lu, Hao Dong, Yaodong Yang

***IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE T-PAMI), 2023***

[Paper](https://ieeexplore.ieee.org/abstract/document/10343126), [Code ![code](https://img.shields.io/github/stars/PKU-MARL/DexterousHands?style=social&label=Code+Stars)](https://github.com/PKU-MARL/DexterousHands)
- A collection of bimanual dexterous manipulations tasks and reinforcement learning algorithms.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE RA-L</div><img src='images/publication/ral23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Learning Semantic-Agnostic and Spatial-Aware Representation for Generalizable Visual-Audio Navigation**

Hongcheng Wang\*, Yuxuan Wang\*, **Fangwei Zhong**, Mingdong Wu, Yizhou Wang, Jianwei Zhang, Hao Dong

***IEEE Robotics and Automation Letters (RA-L), 2023***

[Paper](https://arxiv.org/abs/2304.10773), [Demo](https://www.youtube.com/watch?v=cbNHeKjo3k4)
- A brain-inspired plug-and-play method to learn a semantic-agnostic and spatial-aware representation for generalizable visual-audio navigation.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/publication/cvpr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**GFPose: Learning 3D Human Pose Prior with Gradient Fields**

Hai Ci, Mingdong Wu, Wentao Zhu, Xiaoxuan Ma, Hao Dong, **Fangwei Zhong&#x2709;**, Yizhou Wang

***Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2023***

[Project](https://sites.google.com/view/gfpose/) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:8k81kl-MbHgC'></span></strong>, 
[Paper](https://arxiv.org/abs/2212.08641),
[Code ![code](https://img.shields.io/github/stars/Embracing/GFPose?style=social&label=Code+Stars)](https://github.com/Embracing/GFPose) 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/gfpose-learning-3d-human-pose-prior-with/multi-hypotheses-3d-human-pose-estimation-on)](https://paperswithcode.com/sota/multi-hypotheses-3d-human-pose-estimation-on?p=gfpose-learning-3d-human-pose-prior-with)
- A versatile framework to model plausible 3D human poses in gradient fields for various applications.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/publication/iclr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Proactive Multi-Camera Collaboration for 3D Human Pose Estimation**

Hai Ci\*, Mickel Liu\*, Xuehai Pan\*, **Fangwei Zhong&#x2709;**, Yizhou Wang

***International Conference on Learning Representations (ICLR), 2023***

[Project](https://sites.google.com/view/active3dpose) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:MXK_kJrjxJIC'></span></strong>,
[Paper](https://openreview.net/pdf?id=CPIy9TWFYBG)
- A novel MARL framework to solve proactive multi-camrea collaborations for 3D HPE in human crowds.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/publication/aaai23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking**

**Fangwei Zhong**\*,  Xiao Bi\*,  Yudi Zhang,  Wei Zhang, Yizhou Wang

***Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI), 2023*** <span style="color:red">(Oral)</span><strong><span class='show_paper_citations' data=''></span></strong>

[Project](https://sites.google.com/view/aot-rspt), [Paper](https://arxiv.org/pdf/2304.03623v1.pdf)
- A framework to form a structure-aware motion representation by Reconstructing Surroundings and Predicting the target Trajectory.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/publication/targf_update.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**TarGF: Learning Target Gradient Field to Rearrange Objects without Explicit Goal Specification**

Mingdong Wu\*, **Fangwei Zhong**\*, Yulong Xia, Hao Dong

***Advances in Neural Information Processing Systems (NeurIPS), 2022***

[Project](https://sites.google.com/view/targf) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:0EnyYjriUFMC'></span></strong>,
[Paper](https://arxiv.org/pdf/2209.00853.pdf),
[Code](https://github.com/AaronAnima/TarGF) 
- A framework based on a target gradient field trained by score-matching to tackle object rearrangement without explicit goal specification.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS D&B 2022</div><img src='images/publication/mate.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**MATE: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control**


Xuehai Pan\*, Mickel Liu\*, **Fangwei Zhong&#x2709;**, Yaodong Yang&#x2709;, Song-Chun Zhu, Yizhou Wang

***Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B), 2022***

[Project](https://github.com/UnrealTracking/mate) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:5nxA0vEk-isC'></span></strong>, 
[Paper](https://openreview.net/pdf?id=SyoUVEyzJbE),
[Code ![code](https://img.shields.io/github/stars/UnrealTracking/mate?style=social&label=Code+Stars)](https://github.com/UnrealTracking/mate) 
- A gamification of the multi-camera multi-target target coverage problem, and an all-in-one multi-agent reinforcement learning benchmark
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2022</div><img src='images/publication/icml22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Disentangling Disease-related Representation from Obscure for Disease Prediction**

Chu-Ran Wang, Fei Gao, Fandong Zhang, **Fangwei Zhong&#x2709;**, Yizhou Yu, Yizhou Wang

***International Conference on Machine Learning (ICML), 2022***

[Paper](https://proceedings.mlr.press/v162/wang22f/wang22f.pdf)
- A disentanglement learning strategy under the guidance of alpha blending generation in an encoder-decoder framework (DAB-Net).
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2022</div><img src='images/publication/iclr22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind**

Yuanfei Wang\*, **Fangwei Zhong**\*, Jing Xu, Yizhou Wang

***International Conference on Learning Representations (ICLR), 2022***

[Paper](hhttps://arxiv.org/pdf/2111.09189.pdf), [Code ![code](https://img.shields.io/github/stars/UnrealTracking/ToM2C?style=social&label=Code+Stars)](https://github.com/UnrealTracking/ToM2C) 
- A Target-oriented Multi-agent Communication and Cooperation mechanism using Theory of Mind. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2021</div><img src='images/publication/icml21-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Towards Distraction-Robust Active Visual Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

***International Conference on Machine Learning (ICML), 2021***

[Project](https://sites.google.com/view/distraction-robust-avt),
[Paper](http://proceedings.mlr.press/v139/zhong21b/zhong21b.pdf), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/tree/distractor),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A mixed cooperative-competitive multi-agent game: a target and multiple distractors form a collaborative team to play against a tracker. 
- A bunch of practical methods: a reward function for distractors, a cross-modal teacher-student learning strategy, and a recurrent attention module for the tracker.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2020</div><img src='images/publication/hitmac.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Learning Multi-Agent Coordination for Enhancing Target Coverage in Directional Sensor Networks**

Jing Xu*, **Fangwei Zhong**\*, Yizhou Wang

***Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS), 2021***

[Project](https://sites.google.com/view/hit-mac),
[Paper](https://proceedings.neurips.cc/paper/2020/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf), 
[Code ![code](https://img.shields.io/github/stars/XuJing1022/HiT-MAC?style=social&label=Code+Stars)](https://github.com/XuJing1022/HiT-MAC)
- a Hierarchical Target-oriented Multi-Agent Coordination (HiT-MAC), which decomposes the target coverage problem into two-level tasks: targets assignment by a coordinator and tracking assigned targets by executors. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2020</div><img src='images/publication/aaai20.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

***Pose-Assisted Multi-Camera Collaboration for Active Object Tracking***

Jing Li\*, Jing Xu\*, **Fangwei Zhong***, Xiangyu Kong, Yu Qiao, Yizhou Wang

**Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020**

[Project](https://sites.google.com/view/pose-assisted-collaboration),
[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/5419), 
[Code ![code](https://img.shields.io/github/stars/LilJing/pose-assisted-collaboration?style=social&label=Code+Stars)](https://github.com/LilJing/pose-assisted-collaboration),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv),
[Demo](https://www.youtube.com/watch?v=8Ha7HGkRv6k)
- An efficient yet effective multi-camera collaboration system for collaborative multiCamera active object tracking.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI</div><img src='images/publication/advatplus.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

***IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2021***

[Paper](https://ieeexplore.ieee.org/abstract/document/8896000/), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- Employ more advanced environment augmentation technique and two-stage training strategies to improve the performance of the tracker in the case of challenging scenarios such as obstacles.
- Analyze the target‚Äôs behaviors as the training proceeds and visualize the latent space of the tracker for a better understanding.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2019</div><img src='images/publication/advat.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

***International Conference on Learning Representations (ICLR), 2019***

[Paper](https://openreview.net/pdf?id=HkgYmhR9KX), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A novel adversarial RL method which adopts an Asymmetric Dueling mechanism (tracker vs. target) for robust active visual tracking
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2019</div><img src='images/publication/craves.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**CRAVES: Controlling Robotic Arm with a Vision-based, Economic System**

Yiming Zuo\*, Weichao Qiu\*, Lingxi Xie, **Fangwei Zhong**, Yizhou Wang, Alan L Yuille

***Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2019***

[Project](https://craves.ai/),
[Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zuo_CRAVES_Controlling_Robotic_Arm_With_a_Vision-Based_Economic_System_CVPR_2019_paper.pdf),
[Code ![code](https://img.shields.io/github/stars/zuoym15/craves.ai?style=social&label=Code+Stars)](https://github.com/zuoym15/craves.ai),
[Controller ![code](https://img.shields.io/github/stars/zfw1226/craves_control?style=social&label=Code+Stars)](https://github.com/zfw1226/craves_control),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A vision system for low-cost arm control: trains a vision model in virtual environment, and applies it to real-world images after domain adaptation (a semi-supervised approach).
- One virtual environment for collection data and reinforcement learning.
- Two real-world datasets for evaluation.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI</div><img src='images/publication/e2e-real.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning**

Wenhan Luo\*, Peng Sun\*, **Fangwei Zhong**\*, Wei Liu, Tong Zhang, Yizhou Wang

***IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2020***

[Paper](https://arxiv.org/pdf/1808.03405.pdf),
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- Deploy End-to-end active object tracker trained in virtual environment in real-world robot.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2018</div><img src='images/publication/icml18.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**End-to-end Active Object Tracking via Reinforcement Learning**

Wenhan Luo\*, Peng Sun\*, **Fangwei Zhong**, Wei Liu, Tong Zhang, Yizhou Wang

***International Conference on Machine Learning (ICML), 2018***

[Project](https://sites.google.com/site/whluoimperial/active_tracking_icml2018),
[Paper](http://proceedings.mlr.press/v80/luo18a/luo18a.pdf),
[Training code](https://github.com/whluo/active_tracking_drl),
[Gym-tvizdoom](https://bitbucket.org/pengsun000/gym-tvizdoom/src/master),
[Gym-unrealcv](https://github.com/zfw1226/gym-unrealcv)
- An end-to-end active objet tracking solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WACV 2018</div><img src='images/publication/detect-slam.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Detect-SLAM: Making Object Detection and SLAM Mutually Beneficial**

**Fangwei Zhong**, Sheng Wang, Ziqi Zhang, Chen Zhou, Yizhou Wang

***IEEE Winter Conference on Applications of Computer Vision (WACV), 2018***

[Paper](https://ieeexplore.ieee.org/abstract/document/8354219/),
[Video](https://www.youtube.com/watch?v=eqJiyU9ebaY)
- A robotic vision system which integrates SLAM with a deep neural network-based object detector to make the two functions mutually beneficial.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2017</div><img src='images/publication/unrealcv.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Unrealcv: Virtual worlds for computer vision**

Weichao Qiu, **Fangwei Zhong**, Yi Zhang, Siyuan Qiao, Zihao Xiao, Tae Soo Kim, Yizhou Wang, Alan Yuille

***ACM Multimedia Open Source Software Competition, 2017***

[Project](https://unrealcv.org/),
[Paper](https://dl.acm.org/doi/pdf/10.1145/3123266.3129396),
[Code ![code](https://img.shields.io/github/stars/unrealcv/unrealcv?style=social&label=Code+Stars)](https://github.com/unrealcv/unrealcv),
- An open-sourced project to help computer vision researchers build virtual worlds using Unreal Engine 4 (UE4).
</div>
</div>